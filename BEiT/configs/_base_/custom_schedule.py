# optimizer
lr = 3e-5/2  # max learning rate
optimizer = dict(type='AdamW', lr=3e-5/2, betas=(0.9, 0.999), weight_decay=0.05,
                 constructor='LayerDecayOptimizerConstructor', 
                 paramwise_cfg=dict(num_layers=12, layer_decay_rate=0.9))

lr_config = dict(
    policy='CosineAnnealing',
    warmup='linear',
    warmup_iters=327,
    warmup_ratio=0.1,
    min_lr_ratio=1e-06)
total_epochs = 40
optimizer_config = dict()
